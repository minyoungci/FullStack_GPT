# gpt-3.5-turbo-1106

- 모델
gpt-3.5-turbo-1106 업데이트된 GPT 3.5 Turbo 새로운

- 설명
개선된 지시어 따르기, JSON 모드, 재현 가능한 출력, 병렬 함수 호출 등을 갖춘 최신 GPT-3.5 Turbo 모델입니다. 최대 4,096개의 출력 토큰을 반환합니다. 자세한 내용은 여기에서 확인하세요.

- 가격
플랫폼 전반에 걸쳐 여러 가지 가격을 인하하여 절감된 비용을 개발자에게 전달하고 있습니다 (아래의 모든 가격은 1,000 토큰당 표시됨):

GPT-4 Turbo 입력 토큰은 GPT-4 대비 3배 저렴한 0.01달러이며, 출력 토큰은 2배 저렴한 0.03달러입니다.

GPT-3.5 Turbo 입력 토큰은 이전 16K 모델 대비 3배 저렴한 0.001달러이며, 출력 토큰은 2배 저렴한 0.002달러입니다.

이전에 GPT-3.5 Turbo 4K를 사용하던 개발자는 입력 토큰에 대해 33% 할인된 0.001달러를 이용할 수 있습니다. 이러한 낮은 가격은 오늘 소개된 새로운 GPT-3.5 Turbo에만 적용됩니다.

- 컨텍스트 윈도우
16,385 토큰

- 훈련 데이터
2021년 9월까지의 데이터

# Output parsers

언어 모델은 텍스트를 출력합니다. 그러나 종종 텍스트 이상의 보다 구조화된 정보를 얻고 싶을 수 있습니다. 이 때 출력 파서가 필요합니다.

출력 파서는 언어 모델 응답을 구조화하는 데 도움을 주는 클래스들입니다. 출력 파서는 주로 두 가지 메서드를 구현해야 합니다:

“포맷 지시사항 가져오기”: 언어 모델의 출력이 어떻게 서식이 지정되어야 하는지에 대한 지시사항을 포함한 문자열을 반환하는 메서드입니다.
“구문 분석”: 언어 모델 응답으로 가정되는 문자열을 가져와 어떤 구조로 구문 분석하는 메서드입니다.
그리고 선택적인 메서드 하나:

“프롬프트와 함께 구문 분석”: 언어 모델 응답으로 가정되는 문자열과 (해당 응답을 생성한 것으로 가정되는) 프롬프트를 가져와 어떤 구조로 구문 분석하는 메서드입니다. 이 메서드는 대개 출력을 다시 시도하거나 수정해야 하는 경우에 사용되며, 이를 수행하기 위해 프롬프트에서 정보가 필요한 경우에 프롬프트가 제공됩니다.


# Caching

Streamlit은 사용자 상호 작용이나 코드 변경마다 스크립트를 위에서 아래로 실행합니다. 이 실행 모델은 개발을 매우 쉽게 만들지만 두 가지 주요 문제가 있습니다.

1. 장시간 실행되는 함수가 계속해서 실행되어 앱이 느려지는 문제
2. 객체가 계속 다시 생성되어 다시 실행 또는 세션 간에 그들을 유지하기가 어려운 문제 등이 있습니다.

Streamlit은 내장된 캐싱 메커니즘을 사용하여 이 두 문제에 대처할 수 있게 해줍니다. 캐싱은 느린 함수 호출의 결과를 저장하여 한 번만 실행되면 될 때마다 호출되는 것처럼 동작합니다. 이로써 앱이 훨씬 빨라지며 객체를 다시 실행 간에 유지하는 데 도움이 됩니다.

`@st.cache_data` 로 long_running_function을 장식(decorating)하면 Streamlit에게 함수가 호출될 때 다음 두 가지를 확인하라고 알려줍니다.

1. 입력 매개변수의 값 (이 경우 param1 및 param2).
2. 함수 내부의 코드.

Streamlit이 이러한 매개변수 값과 함수 코드를 처음으로 보는 경우, 함수를 실행하고 반환 값을 캐시에 저장합니다. 다음으로 함수가 동일한 매개변수와 코드로 호출될 때 (예: 사용자가 앱과 상호 작용할 때), Streamlit은 함수 실행을 완전히 건너뛰고 캐시된 값을 반환합니다. 개발 중에는 함수 코드가 변경됨에 따라 캐시가 자동으로 업데이트되어 최신 변경 사항이 캐시에 반영되도록 보장합니다.

언급했듯이 두 가지 캐싱 데코레이터가 있습니다.

1. **st.cache_data** : 데이터를 반환하는 계산을 캐시하는 권장 방법입니다. CSV에서 DataFrame을로드하거나 NumPy 배열을 변환하거나 API를 쿼리하는 등의 함수 (str, int, float, DataFrame, array, list 등)를 캐시합니다. 함수 호출마다 데이터의 새 복사본을 생성하여 변형 및 경쟁 조건에 대비합니다. st.cache_data의 동작은 대부분의 경우에 필요한대로이므로 확실하지 않은 경우 st.cache_data로 시작하고 작동하는지 확인하세요.

2. **st.cache_resource** : ML 모델이나 데이터베이스 연결과 같은 전역 리소스를 캐시하는 권장 방법입니다. 여러 번 로드하고 싶지 않은 직렬화할 수 없는 객체들입니다. 이를 사용하면 리소스를 복사하거나 중복으로 로드하지 않고도 앱의 모든 다시 실행 및 세션에서 이러한 리소스를 공유할 수 있습니다. 캐시된 반환 값에 대한 모든 변이는 직접 캐시된 객체를 직접 변경합니다.

# OpenAI Function calling

LLama와 같은 무료 모델을 사용하는 경우 함수 호출(function calling)을 사용할 수 없습니다. 


함수 호출은 LLM에서 다양한 목적으로 구조화된 출력을 얻는 유용한 방법입니다. "함수"에 대한 스키마를 제공함으로써 LLM은 하나를 선택하고 해당 스키마에 맞는 응답을 출력하려고 노력할 것입니다.

이름이 LLM이 실제로 코드를 실행하고 함수를 호출한다는 것을 시사하고 있지만, 더 정확히 말하면 LLM은 가상 함수가 사용할 인수의 스키마와 일치하는 매개변수를 채우고 있습니다. 이러한 구조화된 응답은 우리가 원하는 목적으로 사용할 수 있습니다!

함수 호출은 LangChain에서 OpenAI Functions 에이전트와 구조화된 출력 체인을 포함한 여러 인기있는 기능의 기본 구성 요소로 작용합니다. 이러한 더 구체적인 사용 사례 외에도 함수 매개변수를 직접 모델에 첨부하고 호출할 수 있습니다.