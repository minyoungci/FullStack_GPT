import streamlit as st 
from langchain.document_loaders import SitemapLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ìˆ˜ì •
def parse_page(soup): # ìš°ë¦¬ê°€ í¬í•¨í•˜ê³  ì‹¶ì€ textë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜. documentì˜ ì „ì²´ HTMLì„ ê°€ì§„ beautiful soup object ê°’ì´ ì—¬ê¸°ì— ì˜¤ê²Œ ë¨.ì¦‰ ì—¬ê¸°ì„œ ê²€ìƒ‰ ê°€ëŠ¥
    header = soup.find("header") # ì„¹ì…˜ì„ ì„ íƒí•  ìˆ˜ ìˆìŒ. header,data,footer ë“±ë“±
    footer = soup.find("footer")
    if header:
        header.decompose() # decompose()ëŠ” tagì™€ contentë¥¼ ë¶„ë¦¬í•´ì„œ ì œê±°

    if footer:
        footer.decompose()
 # print("hello")ì‹œ ì‚¬ì´íŠ¸ì—ì„œ ê°€ì ¸ì˜¨ ê°’ë“¤ì„ ë³´ë©´ Helloê°€ í”„ë¦°íŠ¸ë˜ì–´ ìˆëŠ”ë°, ì´ëŠ” parse_page functionì´ stiemapì˜ ëª¨ë“  URLì— ëŒ€í•´ ì‹¤í–‰ë˜ì—ˆìŒì„ ì˜ë¯¸í•¨.
 # headerì— ìˆëŠ” textë¥¼ ë°˜í™˜í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ.
    return (
        str(soup.get_text())  # get_text()ëŠ” ëª¨ë“  textë¥¼ ë°˜í™˜í•¨. ì „ì²´ í˜ì´ì§€ì—ì„œ header footerë¥¼ ì œê±°í•œ ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ë°˜í™˜
        .replace("\n", " ") # í•„ìš”ì—†ëŠ” ë¶€ë¶„ë“¤ì„ replaceë¡œ ì œê±°(header, footer, ê³µë°± ë¬¸ì, í•„ìš”ì—†ëŠ” ê°ì¢… ë²„íŠ¼ ì œê±°)
        .replace("\xa0", " ")
        .replace("CloseSearch Submit Blog", " ")    
    )


@st.cache_data(show_spinner="Loading Website...") # load_websiteê°€ í•œ ë²ˆ ì‹¤í–‰ëœ í›„ , ê°™ì€ urlë¡œ ë‹¤ì‹œ í˜¸ì¶œë˜ë©´ ì•„ë¬´ ì¼ë„ í•˜ì§€ ì•Šë„ë¡ ì„¤ì •
def load_website(url):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=800,
    chunk_overlap=200,
    )

    loader = SitemapLoader(                  # URL í•„í„°  
        url,
        filter_urls=[
            r"^(.*\/careers\/).", # dataë¥¼ load í•˜ê³ ì‹¶ì€ urlë“¤ì„ ë‹´ì€ list 
        ],
        parsing_function = parse_page         
    ) 
    loader.headers = {"user-agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"}
    loader.request_per_second = 5
    docs = loader.load_and_split(text_splitter=splitter) # load_and_splitì€ ëª¨ë“  URLì„ loadí•˜ê³ , parsing_functionì„ ì ìš©í•œ í›„, text_splitterë¥¼ ì ìš©í•˜ì—¬ textë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì„ ì˜ë¯¸í•¨.
    return docs 

st.set_page_config(
    page_title="SiteGPT",
    page_icon="ğŸ§Š",
)

st.markdown(
    '''
    # Site GPT
    Ask questions about the content of a website.
    Start by writing the URL of the website on the sidebar
    '''
)

st.title("Site GPT")

# ì›¹ ì‚¬ì´íŠ¸ì˜ ëª¨ë“  data, textë¥¼ ìŠ¤í¬ë©í•´ë³´ì 
# ì‚¬ì´ë“œë°”ì—ì„œ URLì„ ì…ë ¥ë°›ì•„ì„œ, ê·¸ URLë¡œë¶€í„° ë°ì´í„°ë¥¼ ë°›ì•„ì˜¤ëŠ” ê²ƒì„ êµ¬í˜„í•´ë³´ì.

# ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŒ.
# 1. playwrightì™€ chromuiumì„ ì‚¬ìš©í•´ì„œ ì›¹ì‚¬ì´íŠ¸ë¥¼ ìŠ¤í¬ë© (jsì½”ë“œê°€ ë§ì€ ì›¹ì‚¬ì´íŠ¸ì˜ ê²½ìš° ìœ ìš©í•¨. ì…€ë ˆë‹ˆì›€ê³¼ ë¹„ìŠ·, ì›¹ ì‚¬ì´íŠ¸ê°€ ëª¨ë“  apië¡œë¶€í„° dataë¥¼ loadí•  ë•Œê¹Œì§€ ì¡°ê¸ˆ ê¸°ë‹¤ë ¤ì•¼í•¨.)

# 2. ì§ì ‘ ê° ì›¹ì‚¬ì´íŠ¸ì˜ sitemapì„ ì¶”ì¶œ

# ë™ì ì¸ ì›¹ì‚¬ì´íŠ¸ëŠ” 1ë²ˆ ë°©ë²• , ì •ì ì¸ ì‚¬ì´íŠ¸ëŠ” 2ë²ˆ ë°©ë²• ì‚¬ìš©.

with st.sidebar:
    url = st.text_input("Write down a URL", placeholder="https://example.com",
                        )
if url:
    if ".xml" not in url:
        with st.sidebar:
            st.error("Please write down a Sitemap URL.")
    else:
        docs = load_website(url)
        st.write(docs)