{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```from langchain.chat_models import ChatOpenAI```\n",
    "\n",
    "chat_models의 ChatOpenAI 모델을 살펴보면 model_name: str = \"gpt-3.5-turbo\" 이렇게 되어있는데, 이는 gpt 3.5 turbo 모델을 사용한다는 뜻입니다.\n",
    "llms.openai는 model_name: str = \"text-davinci-003\" 이렇게 되어있습니다. \n",
    "\n",
    "openai 웹사이트에서(https://platform.openai.com/docs/models/gpt-3-5) 이 모델들의 차이점을 알아볼 수 있습니다. \n",
    "\n",
    "간단하게 둘을 비교해보자면 gpt 3.5 turbo는 text davinci 003 보다 chat에 특화되어 있습니다. 비용이 매우 저렴(1/10 수준)하기 때문입니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 호출 \n",
    "\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source /home/minyoungxi/MINYOUNGXI/fullstack-gpt/env/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API가 통합되어 있어서 위의 langchain에서 사용하고 싶은 모델을 그대로 가져와서 사용할 수 있음. \n",
    "\n",
    "아래의 결과를 보면 대화형인 chatopenai 모델이 조금 더 대화 형식에 가까운 문장을 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\nThere are eight planets in our Solar System: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.',\n",
       " 'As of now, there are eight confirmed planets in our solar system. These include Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. However, there is ongoing debate among astronomers regarding the classification of Pluto as a planet.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "a = llm.predict(\"How many planets are there?\")\n",
    "b = chat.predict(\"How many planets are there?\")\n",
    "\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat model은 단지 질문을 받을 수 있을 뿐만 아니라 대화를 할 수도 있다는 뜻입니다. \n",
    "\n",
    "만약 model의 설정을 바꾸고 싶다면 model의 constructor(생성자)를 통해 할 수 있습니다.\n",
    "\n",
    "ex1. max_tokens = model이 반환하는 결과의 최대 token을 정할 수 있음. \n",
    "ex2. temperature = model이 얼마나 창의적인지를 결정할 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "predict messages는 위에서 본 것처럼 텍스트를 predict 하는 방법입니다. 질문을 하고 답변을 받는 방식이죠. 이번에는 messages들을 predict 할 것입니다. \n",
    "\n",
    "Humanmessage는 우리가 알고 있고, AIMessage는 AI가 보내는 메세지, SystemMessage는 우리가 LLM에 설정들을 제공하기 위한 Message입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! Il mio nome è Paolo. La distanza tra il Messico e la Thailandia è di circa 17.000 chilometri.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content = \"You are a geography expert. And you only reply in Italian.\"),\n",
    "    AIMessage(content = \"Ciao ! mi chiamo Paolo\"),\n",
    "    HumanMessage(content='what is the distance between Mexico and Thailand. Also, what is your name?'),\n",
    "] \n",
    "\n",
    "chat.predict_messages(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates \n",
    "\n",
    "- Prompt는 LLM과 의사소통할 수있는 유일한 방법임.\n",
    "\n",
    "- more custome 해보자 ! \n",
    "\n",
    "- 아래의 코드를 통해 각 구성요소(components)를 잘 익혀보자 !! ( 나중에 어차피 한 라인으로 프롬프트와 템플릿을 사용할 수 있으니 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The distance between Mexico and Thailand is approximately 16,000 kilometers (9,942 miles).'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate , ChatPromptTemplate \n",
    "\n",
    "# PromptTemplate , ChatPromptTemplate  이 두 개의 프롬프트 템플릿은 다르다. \n",
    "# ChatPromptTemplate은 template을 message로부터 만듭니다. 반면 PromptTemplate은 String을 받아서 template을 만듭니다.\n",
    "\n",
    "# example \n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}?\")\n",
    "\n",
    "prompt = template.format(country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "chat.predict(prompt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Γεια σας! Το όνομά μου είναι MIN. Η απόσταση μεταξύ του Μεξικού και της Ταϊλάνδης είναι περίπου 17.000 χιλιόμετρα.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "    (\"ai\", \"Ciao ! mi chiamo {name}\"),\n",
    "    (\n",
    "        \"human\", \n",
    "        \"what is the distance between {country_a} and {country_b}. Also, what is your name?\",\n",
    "    ),\n",
    "] )\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language=\"Greek\",\n",
    "    name=\"MIN\",\n",
    "    country_a=\"Mexico\",\n",
    "    country_b=\"Thailand\",\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parser and LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Red',\n",
       " 'orange',\n",
       " 'yellow',\n",
       " 'green',\n",
       " 'blue',\n",
       " 'indigo',\n",
       " 'violet',\n",
       " 'black',\n",
       " 'white',\n",
       " 'gray']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are a list generationg machine. Everything you are asked will be answered with a comma separated list of max {max_items}. Do NOT reply with anything else\"),\n",
    "    ('human', '{question}')\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items = 10, \n",
    "    question = \"What are the colors?\"  \n",
    ")\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL (LangChainExpressionLanguage)\n",
    "\n",
    "- Lagchain의 전체적인 프로세스와 친해지자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pikachu', 'Charizard', 'Bulbasaur', 'Squirtle', 'Jigglypuff']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are a list generationg machine. Everything you are asked will be answered with a comma separated list of max {max_items}. Do NOT reply with anything else\"),\n",
    "    ('human', '{question}')\n",
    "])\n",
    "\n",
    "chain = template | chat | CommaOutputParser() # langchain의 핵심이다 ! \n",
    "\n",
    "chain.invoke({\"max_items\": 5, \"question\": \"What are the poketmons?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatAnthropic\n__root__\n  Did not find anthropic_api_key, please add an environment variable `ANTHROPIC_API_KEY` which contains it, or pass  `anthropic_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/minyoungxi/MINYOUNGXI/fullstack-gpt/Langchain.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/minyoungxi/MINYOUNGXI/fullstack-gpt/Langchain.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chat \u001b[39m=\u001b[39m ChatAnthropic(temperature\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/minyoungxi/MINYOUNGXI/fullstack-gpt/Langchain.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m chef_prompt \u001b[39m=\u001b[39m ChatPromptTemplate\u001b[39m.\u001b[39mfrom_messages([\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/minyoungxi/MINYOUNGXI/fullstack-gpt/Langchain.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mYou are a world-class international chef. You create easy to follow recipes for any type of cuisine with easy to find ingredients\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/minyoungxi/MINYOUNGXI/fullstack-gpt/Langchain.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mI want to cook \u001b[39m\u001b[39m{cuisine}\u001b[39;00m\u001b[39m food.}\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/minyoungxi/MINYOUNGXI/fullstack-gpt/Langchain.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/minyoungxi/MINYOUNGXI/fullstack-gpt/Langchain.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m chef_chain \u001b[39m=\u001b[39m chef_prompt \u001b[39m|\u001b[39m chat\n",
      "File \u001b[0;32m~/MINYOUNGXI/fullstack-gpt/env/lib/python3.10/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/MINYOUNGXI/fullstack-gpt/env/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatAnthropic\n__root__\n  Did not find anthropic_api_key, please add an environment variable `ANTHROPIC_API_KEY` which contains it, or pass  `anthropic_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "chat = ChatAnthropic(temperature=0.1)\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are a world-class international chef. You create easy to follow recipes for any type of cuisine with easy to find ingredients\"),\n",
    "    ('human', 'I want to cook {cuisine} food.}'),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a vegetarian chef specialized on making traditional recipes vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to recipe it.\"), \n",
    "    (\"human\", \"{recipe}\")\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
