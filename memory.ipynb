{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory \n",
    "\n",
    "- 단순히 이전 대화 내용 전체를 저장하는 메모리입니다. \n",
    "- 단점은 대화 내용이 길어질수록 메모리도 계속 커지기 때문에 비효율적. \n",
    "- 모델 자체에는 메모리가 없기 때문에 우리가 모델에게 요청을 보낼 때 이전 대화 기록 전체를 같이 보내야함. 그래야 모델이 대화 내용을 파악할 수 있음.\n",
    "- 유저와 AI의 대화가 길어질수록 우리가 모델에게 매번 보내야되는 대화 기록이 길어진다는 것입니다. 상당히 비효율적이고 돈이 많이 든다.\n",
    "\n",
    "메모리의 종류는 많고 다르지만 API는 같기 때문에 사용이 편함. ( save_context , load_memory_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True) \n",
    "\n",
    "memory.save_context({\"input\":\"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory\n",
    "\n",
    "- 대화의 특정 부분만 저장하자 ! 범위는 우리가 저장할 수 있고 메모리를 특정 사이즈로 저장할 수 있음 \n",
    "\n",
    "- 챗봇이 전체 대화가 아닌 최근 대화만 기억함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4, # 몇개의 메세지만 저장\n",
    "    )\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "    \n",
    "add_message(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(2,2)\n",
    "add_message(3,3)    \n",
    "add_message(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # k를 4로 설정해서 제일 오래된 1을 버림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory\n",
    "\n",
    "ConversationSummaryMemory 형태의 메모리는 시간이 지남에 따라 대화의 요약을 만듭니다.\n",
    "\n",
    " 이것은 시간이 지남에 따라 대화의 정보를 압축하는 데 유용할 수 있습니다.\n",
    " \n",
    "  대화 요약 메모리는 대화가 진행되는 대로 요약하고 현재 요약 내용을 메모리에 저장합니다. \n",
    "  \n",
    "  그러면 이 메모리를 사용하여 지금까지의 대화의 요약 내용을 프롬프트/체인에 삽입할 수 있습니다. 이 메모리는 과거 메시지의 기록을 프롬프트 축어적 보고에 유지하는 것이 토큰을 너무 많이 차지하는 긴 대화에 가장 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm min\", \"Wow thats good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Korea is so pretty\", \"I wish I could go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Min and the AI responds positively. Min mentions that Korea is pretty and the AI expresses a desire to go there.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory\n",
    "\n",
    "두 가지 아이디어를 결합한 것입니다.\n",
    "\n",
    " 메모리에 최근 상호 작용의 버퍼를 저장하고 있지만 오래된 상호 작용을 완전히 플러싱하는 것이 아니라 요약으로 컴파일하여 두 가지를 모두 사용합니다. \n",
    " \n",
    " 상호 작용을 플러싱할 시점을 결정하기 위해 상호 작용의 수가 아니라 토큰 길이를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm,\n",
    "                                   max_token_limit = 150,\n",
    "                                   return_messages=True,\n",
    "                                   )\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm min\", \"Wow thats good\")\n",
    "add_message(\"Korea is so pretty\", \"I wish I could go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Min and the AI responds positively. Min mentions that Korea is pretty and the AI expresses a desire to go there.')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://python.langchain.com/docs/modules/memory/types/kg\n",
    "\n",
    "### Conversation Knowledge Graph\n",
    "\n",
    "대화 내용 엔티티의 KG(knowledge graph) 를 만들어\n",
    "중요한 요약본을 생성하여 메모리에 저장하는 방식입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas is a person. Nicolas lives in South Korea.')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas is a person. Nicolas lives in South Korea. Nicolas likes kimchi.')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory on LLMChain\n",
    "\n",
    "- memory를 chain에 꽂는 방법과 두 종류의 chain을 사용해서 꽂는 방법 \n",
    "    - LLM chain이라는 것을 사용. off-the-shelf chain인데 직접 chain을 커스텀해서 만들 수 있음.\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m  \n",
      "    you are a helpful AI talking to human.\n",
      "\n",
      "    \n",
      "\n",
      "    Human:My name is min\n",
      "    You\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Min! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI        \n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)   \n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"  \n",
    "    you are a helpful AI talking to human.\n",
    "\n",
    "    {chat_history}\n",
    "\n",
    "    Human:{question}\n",
    "    You\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(llm=llm, \n",
    "                 memory=memory,\n",
    "                 prompt= PromptTemplate.from_template(template),\n",
    "                 verbose=True, # vervose는 chain의 프롬프트 로그들을 확인할 수 있다.\n",
    "                 )\n",
    "\n",
    "chain.predict(question=\"My name is min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m  \n",
      "    you are a helpful AI talking to human.\n",
      "\n",
      "    Human: My name is min\n",
      "AI: Hello Min! How can I assist you today?\n",
      "\n",
      "    Human:I live in South Korea, Seoul\n",
      "    You\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"AI: That's great! Seoul is a vibrant city with a rich history and culture. How can I assist you with anything related to South Korea or Seoul?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in South Korea, Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m  \n",
      "    you are a helpful AI talking to human.\n",
      "\n",
      "    Human: My name is min\n",
      "AI: Hello Min! How can I assist you today?\n",
      "Human: I live in South Korea, Seoul\n",
      "AI: AI: That's great! Seoul is a vibrant city with a rich history and culture. How can I assist you with anything related to South Korea or Seoul?\n",
      "\n",
      "    Human:What is my name?\n",
      "    You\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI: I apologize for the confusion. Your name is Min.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human introduces themselves as Min and mentions that they live in Seoul, South Korea. The AI responds by acknowledging Min and asking how it can assist them. The human then provides information about Seoul, describing it as the capital and largest city of South Korea. They mention its modern architecture, rich history, cultural attractions, popular landmarks, diverse culinary scene, and advanced technology.\\nHuman: What is my name?\\nAI: I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
